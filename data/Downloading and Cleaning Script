config_file_path = 'config.json'

#load the JSON configuration file
with open(config_file_path, 'r') as config_file:
  config = json.load(config_file)

#print the configuration
#Connection_STRING = config['CONNECTION_STRING_AZURE_STORAGE']

CONNECTION_STRING_AZURE_STORAGE = config['connectionString']
CONTAINER_AZURE = 'projectdata'
blob_service_client = BlobServiceClient.from_connection_string(CONNECTION_STRING_AZURE_STORAGE)
container_client = blob_service_client.get_container_client(CONTAINER_AZURE)
ppp_data = []
gdp_data = []
naics = []
blob_list = container_client.list_blobs()
for blob in blob_list:
  if blob.name == "2022_NAICS_Descriptions.xlsx" or blob.name == "SQGDP2__ALL_AREAS_2018_2023.csv":
    if blob.name == "2022_NAICS_Descriptions.xlsx":
      blob_client = container_client.get_blob_client(blob = blob.name)
      blob_data = blob_client.download_blob()
      blob_content = blob_data.readall()
      df = pd.read_excel(blob_content)
      naics.append(df)
    elif blob.name == "SQGDP2__ALL_AREAS_2018_2023.csv":
      blob_client = container_client.get_blob_client(blob = blob.name)
      blob_data = blob_client.download_blob()
      blob_content = blob_data.readall().decode('utf-8')
      df1 = pd.read_csv(StringIO(blob_content))
      gdp_data.append(df1)
  else:
    blob_client = container_client.get_blob_client(blob = blob.name)
    blob_data = blob_client.download_blob()
    blob_content = blob_data.readall().decode('utf-8')
    df2 = pd.read_csv(StringIO(blob_content))
    ppp_data.append(df2)

#-----------------------------------------------------------------------------------------------------------
#To run only a portion of PPP data
batch_size = 3
blob_list = container_client.list_blobs()
for blob in blob_list:
  if blob.name == "2022_NAICS_Descriptions.xlsx" or blob.name == "SQGDP2__ALL_AREAS_2018_2023.csv":
    if blob.name == "2022_NAICS_Descriptions.xlsx":
      continue
     # blob_client = container_client.get_blob_client(blob = blob.name)
     # blob_data = blob_client.download_blob()
     # blob_content = blob_data.readall()
      #df = pd.read_excel(blob_content)
      #naics.append(df)
    elif blob.name == "SQGDP2__ALL_AREAS_2018_2023.csv":
      continue
      #blob_client = container_client.get_blob_client(blob = blob.name)
      #blob_data = blob_client.download_blob()
      #blob_content = blob_data.readall().decode('utf-8')
      #df1 = pd.read_csv(StringIO(blob_content))
      #gdp_data.append(df1)
  else:
    blob_client = container_client.get_blob_client(blob = blob.name)
    blob_data = blob_client.download_blob()
    blob_content = blob_data.readall().decode('utf-8')
    df2 = pd.read_csv(StringIO(blob_content))
    if len(ppp_data) >= batch_size:
      break
    else:
      ppp_data.append(df2)
#------------------------------------------------------------------------------------------------------------

all_naics = pd.concat(naics, ignore_index=True)
all_ppp = pd.concat(ppp_data, ignore_index=True)
all_gdp = pd.concat(gdp_data, ignore_index=True)

#Creating copy of NAICS Table
naics_table = all_naics.copy()
all_naics.shape
#Filtering table to necessary columns only
naics_table = naics_table[['Code','Title']]
#Dropping "T" from the end of strings
naics_table.loc[naics_table['Title'].str[-1] == 'T', 'Title'] = naics_table.loc[naics_table['Title'].str[-1] == 'T', 'Title'].str[:-1]
#Checking to ensure no NA values
naics_table.isna().sum()

#Creating copy of GDP Table
gdp_table = all_gdp.copy()

#Cleaning PPP data
ppp_table = all_ppp.copy()
ppp_table = ppp_table[[
    'BorrowerName', 
    'BorrowerAddress', 
    'BorrowerCity', 
    'BorrowerState', 
    'BorrowerZip', 
    'NAICSCode',
    'LoanNumber',
    'LoanStatusDate',
    'LoanStatus',
    'Term',
    'ProcessingMethod',
    'ServicingLenderName',
    'ServicingLenderAddress',
    'ServicingLenderCity',
    'ServicingLenderState',
    'ServicingLenderZip',
    'BusinessAgeDescription',
    'DateApproved',
    'InitialApprovalAmount',
    'CurrentApprovalAmount',
    'UndisbursedAmount',
    'ForgivenessAmount',
    'OriginatingLender',
    'OriginatingLenderCity',
    'OriginatingLenderState',
    'ForgivenessDate'
    ]]

ppp_table = ppp_table.dropna()
ppp_table.isna().sum()
ppp_table['NAICSCode'] = ppp_table['NAICSCode'].astype('int')
ppp_table['BorrowerZip'] = ppp_table['BorrowerZip'].str.extract(r'^(\d{5})')
ppp_table['ServicingLenderZip'] = ppp_table['ServicingLenderZip'].str.extract(r'^(\d{5})')
ppp_table['LoanStatusDate'] = pd.to_datetime(ppp_table['LoanStatusDate'])
ppp_table['DateApproved'] = pd.to_datetime(ppp_table['DateApproved'])
ppp_table['ForgivenessDate'] = pd.to_datetime(ppp_table['ForgivenessDate'])
ppp_table.head()


all_gdp = pd.concat(gdp_data, ignore_index=True)

gdp_table = all_gdp.copy()
gdp_table = gdp_df.dropna()
gdp_table.iloc[:, 8:] = gdp_table.iloc[:, 8:].apply(pd.to_numeric, errors='coerce')
gdp_table['2018'] = gdp_table['2018:Q1'] + gdp_table['2018:Q2'] + gdp_table['2018:Q3'] + gdp_table['2018:Q4']
gdp_table['2019'] = gdp_table['2019:Q1'] + gdp_table['2019:Q2'] + gdp_table['2019:Q3'] + gdp_table['2019:Q4']
gdp_table['2020'] = gdp_table['2020:Q1'] + gdp_table['2020:Q2'] + gdp_table['2020:Q3'] + gdp_table['2020:Q4']
gdp_table['2021'] = gdp_table['2021:Q1'] + gdp_table['2021:Q2'] + gdp_table['2021:Q3'] + gdp_table['2021:Q4']
gdp_table['2022'] = gdp_table['2022:Q1'] + gdp_table['2022:Q2'] + gdp_table['2022:Q3'] + gdp_table['2022:Q4']
gdp_table['2023'] = gdp_table['2023:Q1'] + gdp_table['2023:Q2'] + gdp_table['2023:Q3'] + gdp_table['2023:Q4']

#Removing spaces at beginning of string
gdp_table['Description'].str.startswith(" ").sum()
mask = gdp_table['Description'].str.startswith(" ")
gdp_table.loc[mask, 'Description'] = gdp_table.loc[mask, 'Description'].str.lstrip()
gdp_table['Description'].str.startswith(" ").sum()
gdp_table = gdp_table.drop('LineCode', axis = 1)

#Creating industry dimension
dim_industry = gdp_table[['IndustryClassification', 'Description']]
dim_industry.rename(columns={'IndustryClassification': 'industry', 'Description': 'industry_classification'}, inplace=True)
dim_industry['industry_id'] = range(1,len(gdp_df) + 1)

dim_industry

dim_location = gdp_df[['GeoFIPS', 'GeoName','Region']]
dim_location = dim_location.rename(columns={'GeoFIPS':'geofips', 'GeoName':'geoname','Region':'region'})
dim_location['location_id'] = range(1,len(gdp_df) + 1)
dim_location

#Creating facts table
facts_table = []
facts_table = gdp_table.iloc[:, 7:37].copy()
facts_table['industry_id'] = dim_industry['industry_id']
facts_table['location_id'] = dim_location['location_id'] 
column_mapping = {
    '2018:Q1': 'q1_2018',
    '2018:Q2': 'q2_2018',
    '2018:Q3': 'q3_2018',
    '2018:Q4': 'q4_2018',
    '2019:Q1': 'q1_2019',
    '2019:Q2': 'q2_2019',
    '2019:Q3': 'q3_2019',
    '2019:Q4': 'q4_2019',
    '2020:Q1': 'q1_2020',
    '2020:Q2': 'q2_2020',
    '2020:Q3': 'q3_2020',
    '2020:Q4': 'q4_2020',
    '2021:Q1': 'q1_2021',
    '2021:Q2': 'q2_2021',
    '2021:Q3': 'q3_2021',
    '2021:Q4': 'q4_2021',
    '2022:Q1': 'q1_2022',
    '2022:Q2': 'q2_2022',
    '2022:Q3': 'q3_2022',
    '2022:Q4': 'q4_2022',
    '2023:Q1': 'q1_2023',
    '2023:Q2': 'q2_2023',
    '2023:Q3': 'q3_2023',
    '2023:Q4': 'q4_2023',
    '2018': 'gdp_2018',
    '2019': 'gdp_2019',
    '2020': 'gdp_2020',
    '2021': 'gdp_2021',
    '2022': 'gdp_2022',
    '2023': 'gdp_2023'
}
facts_table['gdp_id'] = range(1,len(facts_table)+1)
facts_table = facts_table.rename(columns = column_mapping)
facts_table = facts_table.dropna()

pwd = 'cis9440!'
database_url = f'postgresql://group4:{pwd}@cis9440group4.postgres.database.azure.com/postgres'
engine = create_engine(database_url)

facts_table.to_sql('facts_gdp',con=engine, if_exists= 'append', schema = 'gdp_data',index = False)